{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid , save_image\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Alpha Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_folder = './data'\n",
    "for dirname, _, filenames in os.walk(data_folder):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_binaryalphadigits = os.path.join(data_folder, 'binaryalphadigs.mat')\n",
    "data = scipy.io.loadmat(data_binaryalphadigits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_folder=data_folder, file='binaryalphadigs.mat'):\n",
    "    data_file = os.path.join(data_folder, file)\n",
    "\n",
    "    data = scipy.io.loadmat(data_binaryalphadigits)\n",
    "    images = data['dat']\n",
    "    height, width = images[0][0].shape\n",
    "\n",
    "    # Flatten all images across all classes (0-35: 10 digits + 26 letters)\n",
    "    flattened_images = []\n",
    "    for class_images in images:  # Iterate over each class\n",
    "        for img in class_images:  # Iterate over images in the class\n",
    "            flattened_images.append(img.flatten())\n",
    "\n",
    "    # Convert to a tensor\n",
    "    image_tensor = torch.tensor(np.array(flattened_images), dtype=torch.float32)\n",
    "\n",
    "    return image_tensor, images, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a tensor\n",
    "image_tensor, images, height, width = load_data()\n",
    "\n",
    "print(f\"Tensor shape: {image_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def display_image(images, num_images=5):\n",
    "    # Randomly select indices for the images\n",
    "    random_indices = random.sample(range(len(images)), num_images)\n",
    "    \n",
    "    # Set up a grid for the images\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(5, 5))\n",
    "    \n",
    "    # Loop through the random indices and display each image\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        image = images[idx].reshape(20, 16)  # Reshape the image to 20x16 if needed\n",
    "        axes[i].imshow(image, cmap='gray')\n",
    "        axes[i].set_title(f'Image {idx}')\n",
    "        axes[i].axis('off')  # Hide the axes\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(images[0])\n",
    "display_image(images[1])\n",
    "display_image(images[2])\n",
    "display_image(images[3])\n",
    "display_image(images[4])\n",
    "display_image(images[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lire Alpha Digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lire_alpha_digit(data,list_):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X=data['dat'][list_[0]]\n",
    "    len_ = len(list_)\n",
    "\n",
    "    for i in range(1,len_) :\n",
    "        X_i = data['dat'][list_[i]]\n",
    "        X = np.concatenate((X,X_i),axis=0)\n",
    "\n",
    "    n = X.shape[0]\n",
    "    X = np.concatenate(X).reshape((n,320))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = lire_alpha_digit(data, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "from tqdm import tqdm\n",
    "\n",
    "class RBM(nn.Module):\n",
    "\n",
    "    def __init__(self, p: int, q:int, img_size: tuple[int, int]):\n",
    "        super(RBM, self).__init__()\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.height= img_size[0]\n",
    "        self.width= img_size[1]\n",
    "\n",
    "        # Notes:\n",
    "        # nn.Parameters automatically adds the variable to the list of the model's parameters\n",
    "        # nn.Parameter tells Pytorch to include this tensor in the computation graph and compute gradients for it during backprop\n",
    "        # nn.Parameter allow params to move to the right devide when appluing .to(device)\n",
    "\n",
    "        # Parameters\n",
    "        self.W = nn.Parameter(torch.randn(q, p)*1e-2)\n",
    "\n",
    "        # Bias - initialised at 0\n",
    "        self.a = nn.Parameter(torch.zeros(p))\n",
    "        self.b = nn.Parameter(torch.zeros(q))\n",
    "\n",
    "    def entree_sortie(self, v):\n",
    "        # F.linear performs: v.W + b\n",
    "        sigm = torch.sigmoid(F.linear(v, self.W, self.b))\n",
    "        return sigm\n",
    "\n",
    "    def sortie_entree(self, h):\n",
    "        # F.linear performs: h.W(transpose) + a\n",
    "        sigm = torch.sigmoid(F.linear(h, self.W.t(), self.a))\n",
    "        return sigm\n",
    "\n",
    "    def forward(self, v):\n",
    "        raise NotImplementedError(\"Use the train method for training the RBM.\")\n",
    "\n",
    "    def train(self, V, nb_epoch, batch_size, eps=0.001, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the RBM using Contrastive Divergence\n",
    "\n",
    "        Args:\n",
    "        - V: input data\n",
    "        - nb_epoch: Number of epoch\n",
    "        - batch_size: Batch size\n",
    "        - eps: Learning rate\n",
    "        \"\"\"\n",
    "        n = V.size(0)\n",
    "        p, q = self.p, self.q\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(nb_epoch):\n",
    "            # Shuffle dataset\n",
    "            V = V[torch.randperm(n)]\n",
    "            \n",
    "            # Iterate with batch_size step\n",
    "            for j in range(0, n, batch_size):\n",
    "                V_batch = V[j:min(j + batch_size, n)]\n",
    "                batch_size_actual = V_batch.size(0)\n",
    "\n",
    "                v_0 = copy.deepcopy(V_batch)\n",
    "                p_h_v_0 = self.entree_sortie(v_0)\n",
    "                # Sample\n",
    "                h_0 = (torch.rand(batch_size_actual, q) < p_h_v_0).float()\n",
    "                \n",
    "                p_v_h_0 = self.sortie_entree(h_0)\n",
    "                v_1 = (torch.rand(batch_size_actual, p) < p_v_h_0).float()\n",
    "                \n",
    "                p_h_v_1 = self.entree_sortie(v_1)\n",
    "\n",
    "                # grad\n",
    "                grad_a = torch.sum(v_0 - v_1, dim=0)\n",
    "                grad_b = torch.sum(p_h_v_0 - p_h_v_1, dim=0)\n",
    "                grad_W = torch.matmul(v_0.t(), p_h_v_0) - torch.matmul(v_1.t(), p_h_v_1)\n",
    "\n",
    "                # Update params - Normalise to batch size\n",
    "                # Note: We bypass the pytorch computation graph with .data to avoid accumulating gradients\n",
    "                self.W.data += eps * grad_W.t() / batch_size_actual\n",
    "                self.b.data += eps * grad_b / batch_size_actual\n",
    "                self.a.data += eps * grad_a / batch_size_actual\n",
    "\n",
    "            quad_error = self.reconstruction_error(V)\n",
    "            \n",
    "            losses.append(quad_error.item())\n",
    "            if verbose and epoch%10==0:\n",
    "                print(f\"Epoch {epoch+1}/{nb_epoch}, Reconstruction Error (EQ): {quad_error.item():.6f}\")\n",
    "        \n",
    "        self.losses = losses\n",
    "        self.nb_epoch = nb_epoch\n",
    "\n",
    "        if verbose:\n",
    "            self.plot_loss()\n",
    "            print(f\"Final loss: {self.losses[-1]}\")\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def generer_image_RBM(self, num_iterations, num_images):\n",
    "        \"\"\"\n",
    "        Generate samples from an RBM using Gibbs sampling.\n",
    "\n",
    "        Args:\n",
    "            rbm (RBM): The RBM object.\n",
    "            num_iterations (int): Number of Gibbs sampling steps to use.\n",
    "            num_images (int): Number of images to generate.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of generated images.\n",
    "        \"\"\"\n",
    "        # Initialize random visible states\n",
    "        generated_images = torch.bernoulli(torch.rand(num_images, self.p))  # Random binary states\n",
    "\n",
    "        # Perform Gibbs sampling\n",
    "        for _ in range(num_iterations):\n",
    "            # Sample hidden states given visible states\n",
    "            p_h_given_v = self.entree_sortie(generated_images)\n",
    "            h = (torch.rand_like(p_h_given_v) < p_h_given_v).float()\n",
    "\n",
    "            # Sample visible states given hidden states\n",
    "            p_v_given_h = self.sortie_entree(h)\n",
    "            generated_images = (torch.rand_like(p_v_given_h) < p_v_given_h).float()\n",
    "\n",
    "        print(f\"Infos on generated_images:\")\n",
    "        print(f\"type: {type(generated_images)}\")\n",
    "        print(f\"size: {generated_images.size()}\")\n",
    "\n",
    "        # Plot the generated images\n",
    "        _, axes = plt.subplots(1, num_images, figsize=(num_images * 2, 2))\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.imshow(generated_images[i].view(self.height, self.width), cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        return generated_images\n",
    "    \n",
    "    def save_generated_images(self, generated_images: torch.Tensor, path: str, title=None) -> None:\n",
    "\n",
    "        num_images = generated_images.shape[0]\n",
    "\n",
    "        # Plot the generated images\n",
    "        _, axes = plt.subplots(1, num_images, figsize=(num_images * 2, 2))\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.imshow(generated_images[i].view(self.height, self.width), cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        if title is not None:\n",
    "            plt.suptitle(title)\n",
    "\n",
    "        plt.savefig(path)\n",
    "    \n",
    "    def plot_loss(self):\n",
    "\n",
    "        if not hasattr(self, \"losses\"):\n",
    "            raise AttributeError(\"The attribute 'losses' is missing. Make sure to initialize it in runing the train() method before calling plot().\")\n",
    "        \n",
    "        plt.plot(self.losses)\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.title(f'Loss | {self.nb_epoch} epochs')\n",
    "        plt.show()\n",
    "\n",
    "    def reconstruction_error(self, V: torch.Tensor, return_float=False) -> torch.Tensor | float:\n",
    "        \"\"\"Compute the reconstruction error on a set of tensor images\"\"\"\n",
    "        n = V.size(0)\n",
    "        H = self.entree_sortie(V)\n",
    "        V_rec = self.sortie_entree(H)\n",
    "        torch_sum = torch.sum((V - V_rec)**2) / (n*self.p)\n",
    "        if return_float:\n",
    "            return float(torch_sum.item())\n",
    "        return torch_sum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm = RBM(p=image_tensor.size(1), q=64, img_size=(height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rbm_on_dataset(rbm: RBM, data_folder, dataset='binaryalphadigs.mat', nb_epoch=1000, batch_size=10, eps=1e-2, p=256, q=128, verbose=True):\n",
    "    \"\"\"\n",
    "    Function to train the RBM model on the given dataset of binary images.\n",
    "\n",
    "    Args:\n",
    "    - data_folder: Path to the folder containing the 'binaryalphadigs.mat' file.\n",
    "    - nb_epoch: Number of training epochs.\n",
    "    - batch_size: Batch size for training.\n",
    "    - eps: Learning rate for gradient update.\n",
    "    - p: Number of visible units.\n",
    "    - q: Number of hidden units.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    image_tensor, images, height, width = load_data(data_folder, dataset)\n",
    "\n",
    "    # Normalize the data (if necessary)\n",
    "    # Assuming the data is binary, no need to scale between 0 and 1 but if it isn't, you can scale it:\n",
    "    # image_tensor = image_tensor / 255.0  # If data isn't binary, you can normalize\n",
    "\n",
    "    # Initialize RBM model\n",
    "    \n",
    "\n",
    "    # Train the RBM\n",
    "    rbm.train(image_tensor, nb_epoch=nb_epoch, batch_size=batch_size, eps=eps, verbose=verbose)\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rbm_on_dataset(rbm, data_folder=data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume rbm is a trained RBM object\n",
    "generated_images = rbm.generer_image_RBM(num_iterations=100000, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. hyperparameters variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Number of hidden unities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor, images, height, width = load_data(data_folder, 'binaryalphadigs.mat')\n",
    "\n",
    "trained_rbm = []\n",
    "\n",
    "for q in [16, 32, 64, 128, 256]:\n",
    "\n",
    "    rbm = RBM(p=image_tensor.size(1), q=q, img_size=(height, width))\n",
    "    rbm.train(image_tensor, nb_epoch=400, batch_size=10, eps=1e-2, verbose=True)\n",
    "    trained_rbm.append({\"q\":q, \"rbm\": rbm})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_rbm in trained_rbm:\n",
    "    q = q_rbm[\"q\"]\n",
    "    rbm: RBM = q_rbm[\"rbm\"]\n",
    "\n",
    "    print(f\">>>>> q = {q}\")\n",
    "\n",
    "    generated_images = rbm.generer_image_RBM(num_iterations=100000, num_images=10)\n",
    "    error = rbm.reconstruction_error(generated_images, return_float=True)\n",
    "    rbm.save_generated_images(generated_images, path=f\"assets/rbm_q_{q}.png\", title=f\"Nombre d'unités cachées : {q} | Erreur de reconstruction : {round(error, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor, images, height, width = load_data(data_folder, 'binaryalphadigs.mat')\n",
    "\n",
    "trained_rbm = []\n",
    "\n",
    "for batch_size in [1, 5, 10, 50, 100]:\n",
    "\n",
    "    rbm = RBM(p=image_tensor.size(1), q=128, img_size=(height, width))\n",
    "    rbm.train(image_tensor, nb_epoch=400, batch_size=batch_size, eps=1e-2, verbose=True)\n",
    "    trained_rbm.append({\"batch_size\":batch_size, \"rbm\": rbm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rbm_ in trained_rbm:\n",
    "    batch_size = rbm_[\"batch_size\"]\n",
    "    rbm: RBM = rbm_[\"rbm\"]\n",
    "\n",
    "    print(f\">>>>> batch_size = {batch_size}\")\n",
    "\n",
    "    generated_images = rbm.generer_image_RBM(num_iterations=100000, num_images=10)\n",
    "    error = rbm.reconstruction_error(generated_images, return_float=True)\n",
    "    rbm.save_generated_images(generated_images, path=f\"assets/rbm_batchsize_{batch_size}.png\", title=f\"Batch size : {batch_size} | Erreur de reconstruction : {round(error, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Number of Characters to Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(images: np.ndarray) -> torch.Tensor:\n",
    "    # Flatten all images across all classes (0-35: 10 digits + 26 letters)\n",
    "    flattened_images = [img.flatten() for img in images]\n",
    "\n",
    "    # Convert to a tensor\n",
    "    image_tensor = torch.tensor(np.array(flattened_images), dtype=torch.float32)\n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = lire_alpha_digit(data, [10]) # Only A\n",
    "image_tensor = image_to_tensor(images)\n",
    "\n",
    "rbm = RBM(p=image_tensor.size(1), q=128, img_size=(20, 16))\n",
    "\n",
    "_ = rbm.train(image_tensor, nb_epoch=1300, batch_size=10, eps=1e-2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = rbm.generer_image_RBM(num_iterations=1000, num_images=10)\n",
    "error = rbm.reconstruction_error(generated_images, return_float=True)\n",
    "rbm.save_generated_images(generated_images, path=f\"assets/BALD_A.png\", title=f\"Dataset : Binary Alpha Digits - [A] | q : {128} | Erreur de reconstruction : {round(error, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try A, AB, ABC, ABCD... A-Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "list_error = []\n",
    "\n",
    "for i in range(26):\n",
    "    \n",
    "    images = lire_alpha_digit(data, list(range(10, 11+i)))\n",
    "    image_tensor = image_to_tensor(images)\n",
    "\n",
    "    rbm = RBM(p=image_tensor.size(1), q=128, img_size=(20, 16))\n",
    "\n",
    "    rbm.train(image_tensor, nb_epoch=1000, batch_size=10, eps=1e-2, verbose=False)\n",
    "\n",
    "    generated_images = rbm.generer_image_RBM(num_iterations=1000, num_images=10)\n",
    "    error = rbm.reconstruction_error(generated_images, return_float=True)\n",
    "    rbm.save_generated_images(generated_images, path=f\"assets/BALD_A-{alphabet[i]}.png\", title=f\"Dataset : Binary Alpha Digits - [A-{alphabet[i]}] | q : {128} | Erreur de reconstruction : {round(error, 3)}\")\n",
    "\n",
    "    list_error.append(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_mnist_data(data_folder='./data/mnist-dataset') -> tuple[torch.Tensor, np.ndarray, int, int]:\n",
    "\n",
    "    folder_path =Path(data_folder)\n",
    "    train_image_path = folder_path / 'train-images.idx3-ubyte'\n",
    "\n",
    "    with open(train_image_path, 'rb') as file: \n",
    "        data = np.frombuffer(file.read(), dtype = np.uint8)\n",
    "\n",
    "    # values of pixels : [0-255] -> [0, 1]\n",
    "    binarized_data = (data > 127).astype(int)\n",
    "\n",
    "    height, width = 28, 28\n",
    "    images = binarized_data[16:].reshape(-1, height, width)\n",
    "\n",
    "    # Keep only 1404 images like in binaryalphadigs dataset\n",
    "    images: list[np.ndarray] = images[np.random.choice(images.shape[0], 1404, replace=False)]\n",
    "\n",
    "    flattened_images = [img.flatten() for img in images]\n",
    "\n",
    "    # Convert to a tensor\n",
    "    image_tensor = torch.tensor(np.array(flattened_images), dtype=torch.float32)\n",
    "\n",
    "    return image_tensor, images, height, width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor, images, height, width = load_mnist_data()\n",
    "\n",
    "plt.imshow(images[3], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=128\n",
    "rbm = RBM(p=image_tensor.size(1), q=q, img_size=(height, width))\n",
    "\n",
    "_ = rbm.train(image_tensor, nb_epoch=400, batch_size=10, eps=1e-2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = rbm.generer_image_RBM(num_iterations=100000, num_images=10)\n",
    "error = rbm.reconstruction_error(generated_images, return_float=True)\n",
    "rbm.save_generated_images(generated_images, path=f\"assets/mnist.png\", title=f\"Dataset : MNIST | q : {q} | Erreur de reconstruction : {round(error, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_fashion_mnist_data(data_folder='./data/fashionmnist-dataset') -> tuple[torch.Tensor, np.ndarray, int, int]:\n",
    "\n",
    "    folder_path =Path(data_folder)\n",
    "    train_image_path = folder_path / 'train-images-idx3-ubyte'\n",
    "\n",
    "    with open(train_image_path, 'rb') as file: \n",
    "        data = np.frombuffer(file.read(), dtype = np.uint8)\n",
    "\n",
    "    # values of pixels : [0-255] -> [0, 1]\n",
    "    binarized_data = (data > 127).astype(int)\n",
    "\n",
    "    height, width = 28, 28\n",
    "    images = binarized_data[16:].reshape(-1, height, width)\n",
    "\n",
    "    # Keep only 1404 images like in binaryalphadigs dataset\n",
    "    images: list[np.ndarray] = images[np.random.choice(images.shape[0], 1404, replace=False)]\n",
    "\n",
    "    flattened_images = [img.flatten() for img in images]\n",
    "\n",
    "    # Convert to a tensor\n",
    "    image_tensor = torch.tensor(np.array(flattened_images), dtype=torch.float32)\n",
    "\n",
    "    return image_tensor, images, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor, images, height, width = load_fashion_mnist_data()\n",
    "\n",
    "plt.imshow(images[1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=128\n",
    "rbm = RBM(p=image_tensor.size(1), q=q, img_size=(height, width))\n",
    "\n",
    "_ = rbm.train(image_tensor, nb_epoch=400, batch_size=10, eps=1e-2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = rbm.generer_image_RBM(num_iterations=100000, num_images=10)\n",
    "error = rbm.reconstruction_error(generated_images, return_float=True)\n",
    "rbm.save_generated_images(generated_images, path=f\"assets/fashion_mnist.png\", title=f\"Dataset : Fashion MNIST | q : {q} | Erreur de reconstruction : {round(error, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Try other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GAN (Not fully functional)\n",
    "\n",
    "based on the following github repository : https://github.com/sssingh/mnist-digit-generation-gan.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "\n",
    "data_folder = \"../data\"\n",
    "\n",
    "\n",
    "def create_dataset(data_folder=data_folder, file='binaryalphadigs.mat'):\n",
    "    \n",
    "    data_file = os.path.join(data_folder, file)\n",
    "    data = scipy.io.loadmat(data_file)\n",
    "    images = data['dat']\n",
    "\n",
    "    image_array = np.array([img.tolist() for img in images.flatten()])\n",
    "    image_tensor = torch.tensor(image_array, dtype=torch.float32)\n",
    "\n",
    "    targets = torch.tensor([[c]*images.shape[1] for c in range(images.shape[0])])\n",
    "    targets = targets.flatten()\n",
    "\n",
    "    return image_tensor, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "image_tensor, targets = create_dataset()\n",
    "\n",
    "dataset = TensorDataset(image_tensor, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloader\n",
    "dl = DataLoader(dataset=dataset,\n",
    "                shuffle=True,\n",
    "                batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a sample batch from the dataloader\n",
    "image_batch = next(iter(dl))\n",
    "print(len(image_batch), type(image_batch))\n",
    "print(image_batch[0].shape)\n",
    "print(image_batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Visualise a sample batch\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "def display_images(images, n_cols=4, figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    Utility function to display a collection of images in a grid\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    images: Tensor\n",
    "            tensor of shape (batch_size, channel, height, width)\n",
    "            containing images to be displayed\n",
    "    n_cols: int\n",
    "            number of columns in the grid\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.style.use('ggplot')\n",
    "    n_images = len(images)\n",
    "    n_rows = math.ceil(n_images / n_cols)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for idx in range(n_images):\n",
    "        ax = plt.subplot(n_rows, n_cols, idx+1)\n",
    "        image = images[idx]\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_images(images=image_batch[0], n_cols=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Discriminator will down-sample the input producing a binary output\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "        self.leaky_relu1 = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.leaky_relu2 = nn.LeakyReLU(negative_slope=0.2)        \n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.leaky_relu3 = nn.LeakyReLU(negative_slope=0.2)        \n",
    "        self.fc4 = nn.Linear(in_features=32, out_features=out_features)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Rehape passed image batch\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        # Feed forward\n",
    "        x = self.fc1(x)\n",
    "        x = self.leaky_relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.leaky_relu2(x)\n",
    "        x = self.dropout(x)                        \n",
    "        x = self.fc3(x)\n",
    "        x = self.leaky_relu3(x)        \n",
    "        x = self.dropout(x)\n",
    "        logit_out = self.fc4(x)\n",
    "        \n",
    "        return logit_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Generator, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Generator will up-sample the input producing input of size\n",
    "        # suitable for feeding into discriminator\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=32)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=64)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.2)        \n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.relu3 = nn.LeakyReLU(negative_slope=0.2)        \n",
    "        self.fc4 = nn.Linear(in_features=128, out_features=out_features)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Feed forward\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        tanh_out = self.tanh(x)\n",
    "        \n",
    "        return tanh_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(predicted_outputs, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Function for calculating loss when samples are drawn from real dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted_outputs: Tensor\n",
    "                       predicted logits\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    real_loss: int\n",
    "    \"\"\"\n",
    "    batch_size = predicted_outputs.shape[0]\n",
    "    # Targets are set to 1 here because we expect prediction to be \n",
    "    # 1 (or near 1) since samples are drawn from real dataset\n",
    "    targets = torch.ones(batch_size).to(device)\n",
    "    real_loss = loss_fn(predicted_outputs.squeeze(), targets)\n",
    "    \n",
    "    return real_loss\n",
    "\n",
    "\n",
    "def fake_loss(predicted_outputs, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Function for calculating loss when samples are generated fake samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted_outputs: Tensor\n",
    "                       predicted logits\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    fake_loss: int\n",
    "    \"\"\"\n",
    "    batch_size = predicted_outputs.shape[0]\n",
    "    # Targets are set to 0 here because we expect prediction to be \n",
    "    # 0 (or near 0) since samples are generated fake samples\n",
    "    targets = torch.zeros(batch_size).to(device)\n",
    "    fake_loss = loss_fn(predicted_outputs.squeeze(), targets)\n",
    "    \n",
    "    return fake_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop function\n",
    "\n",
    "def train_gan(d, g, d_optim, g_optim, loss_fn, dl, n_epochs, device, verbose=False):\n",
    "    print(f'Training on [{device}]...')\n",
    "    \n",
    "    # Generate a batch (say 16) of latent image vector (z) of fixed size \n",
    "    # (say 100 pix) to be as input to the Generator after each epoch of \n",
    "    # training to generate a fake image. We'll visualise these fake images\n",
    "    # to get a sense how generator improves as training progresses\n",
    "    z_size = 100\n",
    "    fixed_z = (np.random.uniform(-1, 1, size=(16, z_size)) > 0.5).astype(np.int32)\n",
    "    fixed_z = torch.from_numpy(fixed_z).float().to(device)\n",
    "    fixed_samples = []\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    \n",
    "    \n",
    "    # Move discriminator and generator to available device\n",
    "    d = d.to(device)\n",
    "    g = g.to(device)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]:')\n",
    "        # Switch the training mode on\n",
    "        d.train()\n",
    "        g.train()\n",
    "        d_running_batch_loss = 0\n",
    "        g_running_batch_loss = 0\n",
    "        for curr_batch, (real_images, _) in enumerate(dl):\n",
    "            # Move input batch to available device\n",
    "            real_images = real_images.to(device)\n",
    "            \n",
    "            ## ----------------------------------------------------------------\n",
    "            ## Train discriminator using real and then fake MNIST images,  \n",
    "            ## then compute the total-loss and back-propogate the total-loss\n",
    "            ## ----------------------------------------------------------------\n",
    "            \n",
    "            # Reset gradients\n",
    "            d_optim.zero_grad()\n",
    "            \n",
    "            # Real MNIST images\n",
    "            # Convert real_images value range of 0 to 1 to -1 to 1\n",
    "            # this is required because latter discriminator would be required \n",
    "            # to consume generator's 'tanh' output which is of range -1 to 1\n",
    "            real_images = (real_images * 2) - 1  \n",
    "            d_real_logits_out = d(real_images)\n",
    "            d_real_loss = real_loss(d_real_logits_out, loss_fn, device)\n",
    "            #d_real_loss = real_loss(d_real_logits_out, smooth=True)\n",
    "            \n",
    "            # Fake images\n",
    "            with torch.no_grad():\n",
    "                # Generate a batch of random latent vectors \n",
    "                z = (np.random.uniform(-1, 1, size=(dl.batch_size, z_size)) > 0.5).astype(np.int32)\n",
    "                z = torch.from_numpy(z).float().to(device)\n",
    "                # Generate batch of fake images\n",
    "                fake_images = g(z) \n",
    "            # feed fake-images to discriminator and compute the \n",
    "            # fake_loss (i.e. target label = 0)\n",
    "            d_fake_logits_out = d(fake_images)\n",
    "            d_fake_loss = fake_loss(d_fake_logits_out, loss_fn, device)\n",
    "            #d_fake_loss = fake_loss(d_fake_logits_out)\n",
    "            # Compute total discriminator loss\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            # Backpropogate through discriminator\n",
    "            d_loss.backward()\n",
    "            d_optim.step()\n",
    "            # Save discriminator batch loss\n",
    "            d_running_batch_loss += d_loss\n",
    "            \n",
    "            ## ----------------------------------------------------------------\n",
    "            ## Train generator, compute the generator loss which is a measure\n",
    "            ## of how successful the generator is in tricking the discriminator \n",
    "            ## and finally back-propogate generator loss\n",
    "            ## ----------------------------------------------------------------\n",
    "\n",
    "            # Reset gradients\n",
    "            g_optim.zero_grad()\n",
    "            \n",
    "            # Generate a batch of random latent vectors\n",
    "            #z = torch.rand(size=(dl.batch_size, z_size)).to(device)\n",
    "            z = (np.random.uniform(-1, 1, size=(dl.batch_size, z_size)) > 0.5).astype(np.int32)\n",
    "            z = torch.from_numpy(z).float().to(device)       \n",
    "            # Generate a batch of fake images, feed them to discriminator\n",
    "            # and compute the generator loss as real_loss \n",
    "            # (i.e. target label = 1)\n",
    "            fake_images = g(z) \n",
    "            g_logits_out = d(fake_images)\n",
    "            g_loss = real_loss(g_logits_out, loss_fn, device)\n",
    "            #g_loss = real_loss(g_logits_out)\n",
    "            # Backpropogate thorugh generator\n",
    "            g_loss.backward()\n",
    "            g_optim.step()\n",
    "            # Save discriminator batch loss\n",
    "            g_running_batch_loss += g_loss\n",
    "            \n",
    "            # Display training stats for every 200 batches \n",
    "            if curr_batch % 400 == 0 and verbose:\n",
    "                print(f'\\tBatch [{curr_batch:>4}/{len(dl):>4}] - d_batch_loss: {d_loss.item():.6f}\\tg_batch_loss: {g_loss.item():.6f}')\n",
    "            \n",
    "        # Compute epoch losses as total_batch_loss/number_of_batches\n",
    "        d_epoch_loss = d_running_batch_loss.item()/len(dl)\n",
    "        g_epoch_loss = g_running_batch_loss.item()/len(dl)\n",
    "        d_losses.append(d_epoch_loss)\n",
    "        g_losses.append(g_epoch_loss)\n",
    "        \n",
    "        # Display training stats for every 200 batches \n",
    "        print(f'epoch_d_loss: {d_epoch_loss:.6f} \\tepoch_g_loss: {g_epoch_loss:.6f}')\n",
    "        \n",
    "        # Generate fake images from fixed latent vector using the trained \n",
    "        # generator so far and save images for latter viewing\n",
    "        g.eval()\n",
    "        fixed_samples.append(g(fixed_z).detach().cpu())\n",
    "        \n",
    "    # Finally write generated fake images from fixed latent vector to disk\n",
    "    with open('fixed_samples.pkl', 'wb') as f:\n",
    "        pkl.dump(fixed_samples, f)\n",
    "     \n",
    "    return d_losses, g_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Prepare and start training\n",
    "##\n",
    "\n",
    "# Instantiate Discriminator and Generator\n",
    "d = Discriminator(in_features=784, out_features=1)\n",
    "g = Generator(in_features=100, out_features=784)\n",
    "#g = Generator(100, 32, 784)\n",
    "print(d)\n",
    "print()\n",
    "print(g)\n",
    "\n",
    "# Instantiate optimizers\n",
    "d_optim = optim.Adam(d.parameters(), lr=0.002)\n",
    "g_optim = optim.Adam(g.parameters(), lr=0.002)\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#Train\n",
    "n_epochs = 100\n",
    "d_losses, g_losses = train_gan(d, g, d_optim, g_optim, \n",
    "                                     loss_fn, dl, n_epochs, device,\n",
    "                                     verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
