{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid , save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module):\n",
    "\n",
    "    def __init__(self, p: int, q:int):\n",
    "        super(RBM, self).__init__()\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "\n",
    "        # Notes:\n",
    "        # nn.Parameters automatically adds the variable to the list of the model's parameters\n",
    "        # nn.Parameter tells Pytorch to include this tensor in the computation graph and compute gradients for it during backprop\n",
    "        # nn.Parameter allow params to move to the right devide when appluing .to(device)\n",
    "\n",
    "        # Parameters\n",
    "        self.W = nn.Parameter(torch.randn(q, p)*1e-2)\n",
    "\n",
    "        # Bias - initialised at 0\n",
    "        self.a = nn.Parameter(torch.zeros(p))\n",
    "        self.b = nn.Parameter(torch.zeros(q))\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def entree_sortie(self, v):\n",
    "        # F.linear performs: v.W + b\n",
    "        sigm = torch.sigmoid(F.linear(v, self.W, self.b))\n",
    "        return sigm\n",
    "\n",
    "    def sortie_entree(self, h):\n",
    "        # F.linear performs: h.W(transpose) + a\n",
    "        sigm = torch.sigmoid(F.linear(h, self.W.t(), self.a))\n",
    "\n",
    "    def forward(self, v):\n",
    "        raise NotImplementedError(\"Use the train method for training the RBM.\")\n",
    "\n",
    "    def train(self, V, nb_epoch, batch_size, eps=0.001):\n",
    "        \"\"\"\n",
    "        Train the RBM using Contrastive Divergence\n",
    "\n",
    "        Args:\n",
    "        - V: input data\n",
    "        - nb_epoch: Number of epoch\n",
    "        - batch_size: Batch size\n",
    "        - eps: Learning rate\n",
    "        \"\"\"\n",
    "        n = V.size(0)\n",
    "        p, q = self.p, self.q\n",
    "\n",
    "        for epoch in range(nb_epoch):\n",
    "            # Shuffle dataset\n",
    "            V = V[torch.randperm(n)]\n",
    "            \n",
    "            # Iterate with batch_size step\n",
    "            for j in range(0, n, batch_size):\n",
    "                V_batch = V[j:min(j + batch_size, n)]\n",
    "                batch_size_actual = V_batch.size(0)\n",
    "\n",
    "                v_0 = V_batch\n",
    "                p_h_v_0 = self.entree_sortie(v_0)\n",
    "                # Sample\n",
    "                h_0 = (torch.rand(batch_size_actual, q) < p_h_v_0)*1\n",
    "                \n",
    "                p_v_h_0 = self.sortie_entree(h_0)\n",
    "                v_1 = (torch.rand(batch_size_actual, p) < p_v_h_0)*1\n",
    "                \n",
    "                p_h_v_1 = self.entree_sortie(v_1)\n",
    "\n",
    "                # grad\n",
    "                grad_a = torch.sum(v_0 - v_1, dim=0)\n",
    "                grad_b = torch.sum(p_h_v_0 - p_h_v_1, dim=0)\n",
    "                grad_W = torch.matmul(v_0.t(), p_h_v_0) - torch.matmul(v_1.t(), p_h_v_1)\n",
    "\n",
    "                # Update params - Normalise to batch size\n",
    "                # Note: We bypass the pytorch computation graph with .data to avoid accumulating gradients\n",
    "                self.W.data += eps * grad_W.t() / batch_size_actual\n",
    "                self.b.data += eps * grad_b / batch_size_actual\n",
    "                self.a.data += eps * grad_a / batch_size_actual\n",
    "\n",
    "            H = self.entree_sortie(V)\n",
    "            V_rec = self.sortie_entree(H)\n",
    "            quad_error = torch.sum((V - V_rec)**2) / (n*p)\n",
    "            print(f\"Epoch {epoch+1}/{nb_epoch}, Reconstruction Error (EQ): {quad_error.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
